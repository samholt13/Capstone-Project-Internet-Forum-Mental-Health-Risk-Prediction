{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:06:16.008303Z",
     "start_time": "2020-06-03T16:06:15.947952Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as scipy\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, StratifiedKFold, GridSearchCV, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:06:20.246509Z",
     "start_time": "2020-06-03T16:06:16.010315Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/samholt/GA/DSI12-lessons/projects/Capstone_Project/Capstone_MKII_DataProcessed.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:06:20.497412Z",
     "start_time": "2020-06-03T16:06:20.248660Z"
    }
   },
   "outputs": [],
   "source": [
    "#set index to unnamed\n",
    "df.drop(\"Unnamed: 0\", axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:06:20.526836Z",
     "start_time": "2020-06-03T16:06:20.499376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iphone</th>\n",
       "      <th>news</th>\n",
       "      <th>gaming</th>\n",
       "      <th>DestinyTheGame</th>\n",
       "      <th>MonsterHunter</th>\n",
       "      <th>single_user_subreddits</th>\n",
       "      <th>The_Donald</th>\n",
       "      <th>NoStupidQuestions</th>\n",
       "      <th>Stims</th>\n",
       "      <th>nba</th>\n",
       "      <th>...</th>\n",
       "      <th>AskReddit</th>\n",
       "      <th>FortNiteBR</th>\n",
       "      <th>CircleofTrust</th>\n",
       "      <th>Showerthoughts</th>\n",
       "      <th>fo76</th>\n",
       "      <th>BrandNewSentence</th>\n",
       "      <th>inthesoulstone</th>\n",
       "      <th>pokemongotrades</th>\n",
       "      <th>PushaT</th>\n",
       "      <th>df_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   iphone  news  gaming  DestinyTheGame  MonsterHunter  \\\n",
       "0     0.0   0.0     0.0             0.0            0.0   \n",
       "1     0.0   0.0     0.0             0.0            0.0   \n",
       "2     0.0   0.0     0.0             0.0            0.0   \n",
       "3     0.0   0.0     0.0             0.0            0.0   \n",
       "4     0.0   0.0     0.0             0.0            0.0   \n",
       "\n",
       "   single_user_subreddits  The_Donald  NoStupidQuestions  Stims  nba  ...  \\\n",
       "0                     0.0         0.0                0.0    0.0  0.0  ...   \n",
       "1                     1.0         0.0                0.0    0.0  0.0  ...   \n",
       "2                     0.0         0.0                0.0    0.0  0.0  ...   \n",
       "3                     0.0         0.0                0.0    0.0  0.0  ...   \n",
       "4                     1.0         0.0                0.0    0.0  0.0  ...   \n",
       "\n",
       "   AskReddit  FortNiteBR  CircleofTrust  Showerthoughts  fo76  \\\n",
       "0        1.0         0.0            0.0             0.0   0.0   \n",
       "1        0.0         0.0            0.0             0.0   0.0   \n",
       "2        0.0         0.0            0.0             0.0   0.0   \n",
       "3        0.0         0.0            0.0             0.0   0.0   \n",
       "4        0.0         0.0            0.0             0.0   0.0   \n",
       "\n",
       "   BrandNewSentence  inthesoulstone  pokemongotrades  PushaT  df_target  \n",
       "0               0.0             0.0              0.0     0.0        0.0  \n",
       "1               0.0             0.0              0.0     0.0        0.0  \n",
       "2               0.0             0.0              0.0     0.0        0.0  \n",
       "3               0.0             0.0              0.0     0.0        0.0  \n",
       "4               0.0             0.0              0.0     0.0        0.0  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:06:21.477378Z",
     "start_time": "2020-06-03T16:06:20.528264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 80000 entries, 0 to 79999\n",
      "Columns: 501 entries, iphone to df_target\n",
      "dtypes: float64(501)\n",
      "memory usage: 305.8 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof of Concept modelling\n",
    "* Testing models on sampled data ahead of porting to AWS for greater computational power\n",
    "* Creating class function to assess various resampling methods on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:06:22.187655Z",
     "start_time": "2020-06-03T16:06:21.479650Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split sample into target and predictor variables\n",
    "y = df.pop(\"df_target\")\n",
    "X = df\n",
    "\n",
    "X\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:06:24.246885Z",
     "start_time": "2020-06-03T16:06:22.189197Z"
    }
   },
   "outputs": [],
   "source": [
    "# create train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify= y, random_state=13)\n",
    "\n",
    "# standardize data, maxabsscaler as to work on sparse data \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:06:24.277357Z",
     "start_time": "2020-06-03T16:06:24.251731Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dataframe to track params & scoring\n",
    "cols = [\"phase\", \"model_type\", \"cv_score\", \"training_score\", \"precision\", \"recall\", \"f1-score\", \"TP\", \"FP\", \"TN\", \"FN\"]\n",
    "model_tracker = pd.DataFrame(columns= cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:06:24.295024Z",
     "start_time": "2020-06-03T16:06:24.281770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phase</th>\n",
       "      <th>model_type</th>\n",
       "      <th>cv_score</th>\n",
       "      <th>training_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [phase, model_type, cv_score, training_score, precision, recall, f1-score, TP, FP, TN, FN]\n",
       "Index: []"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.790282Z",
     "start_time": "2020-06-03T16:11:06.541Z"
    }
   },
   "outputs": [],
   "source": [
    "#create function to run different models\n",
    "def model_run(model):\n",
    "    \"\"\"returns average score, training score, class report metrics and confusion matrix outputs\"\"\"\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    # get cross val score\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "\n",
    "    # get training score\n",
    "    model.fit(X_train, y_train)\n",
    "    training_score = model.score(X_train, y_train)\n",
    "\n",
    "    # predict and get classification report & confusion matrix info\n",
    "    predictions = model.predict(X_test)\n",
    "    class_report = classification_report(y_test, predictions, output_dict= True)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    \n",
    "    print(\"Model run complete\")\n",
    "    return scores, training_score, class_report, tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.791229Z",
     "start_time": "2020-06-03T16:11:06.548Z"
    }
   },
   "outputs": [],
   "source": [
    "# logistic regression, base model\n",
    "model = LogisticRegression(class_weight= \"balanced\", solver = \"sag\", max_iter= 8000)\n",
    "\n",
    "scores, training_score, class_report, tn, fp, fn, tp = model_run(model)\n",
    "\n",
    "# append to model tracker\n",
    "results = pd.Series([\"PoC\", \"log_reg\", scores.mean(), training_score, class_report[\"1.0\"][\"precision\"], class_report[\"1.0\"][\"recall\"], class_report[\"1.0\"][\"f1-score\"],tp, fp, tn, fn], index= model_tracker.columns)\n",
    "model_tracker = model_tracker.append(results, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.792400Z",
     "start_time": "2020-06-03T16:11:06.549Z"
    }
   },
   "outputs": [],
   "source": [
    "# KNN, base model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "scores, training_score, class_report, tn, fp, fn, tp = model_run(model)\n",
    "\n",
    "results = pd.Series([\"PoC\", \"KNN\", scores.mean(), training_score, class_report[\"1.0\"][\"precision\"], class_report[\"1.0\"][\"recall\"], class_report[\"1.0\"][\"f1-score\"],tp, fp, tn, fn], index= model_tracker.columns)\n",
    "model_tracker = model_tracker.append(results, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.794023Z",
     "start_time": "2020-06-03T16:11:06.551Z"
    }
   },
   "outputs": [],
   "source": [
    "# Decision tree, base model\n",
    "model = DecisionTreeClassifier(min_samples_split= 50)\n",
    "\n",
    "scores, training_score, class_report, tn, fp, fn, tp = model_run(model)\n",
    "\n",
    "results = pd.Series([\"PoC\", \"D_Tree\", scores.mean(), training_score, class_report[\"1.0\"][\"precision\"], class_report[\"1.0\"][\"recall\"], class_report[\"1.0\"][\"f1-score\"],tp, fp, tn, fn], index= model_tracker.columns)\n",
    "model_tracker = model_tracker.append(results, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.795488Z",
     "start_time": "2020-06-03T16:11:06.555Z"
    }
   },
   "outputs": [],
   "source": [
    "# Random forest, basic model\n",
    "model = RandomForestClassifier(n_estimators= 100, n_jobs= 2)\n",
    "\n",
    "scores, training_score, class_report, tn, fp, fn, tp = model_run(model)\n",
    "\n",
    "results = pd.Series([\"PoC\", \"Rand_Fr\", scores.mean(), training_score, class_report[\"1.0\"][\"precision\"], class_report[\"1.0\"][\"recall\"], class_report[\"1.0\"][\"f1-score\"],tp, fp, tn, fn], index= model_tracker.columns)\n",
    "model_tracker = model_tracker.append(results, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.796853Z",
     "start_time": "2020-06-03T16:11:06.557Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gradient boost, basic model\n",
    "model = model = GradientBoostingClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=1)\n",
    "\n",
    "scores, training_score, class_report, tn, fp, fn, tp = model_run(model)\n",
    "\n",
    "results = pd.Series([\"PoC\", \"Grad_Bst\", scores.mean(), training_score, class_report[\"1.0\"][\"precision\"], class_report[\"1.0\"][\"recall\"], class_report[\"1.0\"][\"f1-score\"],tp, fp, tn, fn], index= model_tracker.columns)\n",
    "model_tracker = model_tracker.append(results, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.798496Z",
     "start_time": "2020-06-03T16:11:06.559Z"
    }
   },
   "outputs": [],
   "source": [
    "# Naive bayes, basic model\n",
    "model = MultinomialNB()\n",
    "\n",
    "scores, training_score, class_report, tn, fp, fn, tp = model_run(model)\n",
    "\n",
    "results = pd.Series([\"PoC\", \"M_NB\", scores.mean(), training_score, class_report[\"1.0\"][\"precision\"], class_report[\"1.0\"][\"recall\"], class_report[\"1.0\"][\"f1-score\"],tp, fp, tn, fn], index= model_tracker.columns)\n",
    "model_tracker = model_tracker.append(results, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.799841Z",
     "start_time": "2020-06-03T16:11:06.561Z"
    }
   },
   "outputs": [],
   "source": [
    "# Linear SVC, basic model\n",
    "model = LinearSVC()\n",
    "\n",
    "scores, training_score, class_report, tn, fp, fn, tp = model_run(model)\n",
    "\n",
    "results = pd.Series([\"PoC\", \"SVC\", scores.mean(), training_score, class_report[\"1.0\"][\"precision\"], class_report[\"1.0\"][\"recall\"], class_report[\"1.0\"][\"f1-score\"],tp, fp, tn, fn], index= model_tracker.columns)\n",
    "model_tracker = model_tracker.append(results, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.800929Z",
     "start_time": "2020-06-03T16:11:06.563Z"
    }
   },
   "outputs": [],
   "source": [
    "model_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.802062Z",
     "start_time": "2020-06-03T16:11:06.566Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualisation of accuracy vs. recall for these models show a distinct trade-off, the model(s) to\n",
    "# take forward are the ones which minimize the accuracy recall trade-off\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "sns.scatterplot(model_tracker[\"recall\"], model_tracker[\"accuracy\"], hue= model_tracker[\"Model\"])\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess Optimal Sampling Technique with Various Models\n",
    "* To improve the models in terms of identifying the minority class, several under and oversampling methodologies were applied to the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.802956Z",
     "start_time": "2020-06-03T16:11:06.573Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import (RandomUnderSampler, \n",
    "                                     ClusterCentroids,\n",
    "                                     TomekLinks,\n",
    "                                     NeighbourhoodCleaningRule,\n",
    "                                     NearMiss)\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.804070Z",
     "start_time": "2020-06-03T16:11:06.577Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dataframe to track params & scoring\n",
    "cols = [\"Model\", \"Method\", \"cv_score\", \"training_score\", \"accuracy\", \"precision\", \"recall\", \"f1-score\", \"TP\", \"FP\", \"TN\", \"FN\"]\n",
    "model_tracker = pd.DataFrame(columns= cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.805318Z",
     "start_time": "2020-06-03T16:11:06.579Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating class called tiny target which will evaluate several different sampling techniques on a user selected model\n",
    "class tiny_target:\n",
    "    def __init__(self, X, y, scaler= StandardScaler(), model= LogisticRegression(), sampling_strategy= 0.5, train_size = 0.3, run_all = True, cv_scoring = \"recall\", stratify = y, cv= 5, tracker = None, name = \"model\"):\n",
    "        if tracker is not None:\n",
    "            self.tracker = tracker\n",
    "        else:\n",
    "            cols = [\"Model\", \"Method\", \"cv_score\", \"training_score\", \"accuracy\", \"precision\", \"recall\", \"f1-score\", \"TP\", \"FP\", \"TN\", \"FN\"]\n",
    "            model_tracker = pd.DataFrame(columns= cols)\n",
    "            self.tracker = model_tracker\n",
    "            \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.model = model\n",
    "        self.sampling_strategy = sampling_strategy\n",
    "        self.train_size = train_size\n",
    "        self.stratify = stratify\n",
    "        self.cv = cv\n",
    "        self.name = name\n",
    "        self.cv_scoring = cv_scoring\n",
    "        self.tracker = tracker\n",
    "    \n",
    "    def model_runner(self):\n",
    "        \"\"\"runs models and appends various scoring metrics (y target focused) to model tracker\"\"\"\n",
    "        self.model.fit(self.X_train_new, self.y_train_new)\n",
    "        self.scores = cross_val_score(self.model, self.X_train_new, self.y_train_new, cv= self.cv, scoring = self.cv_scoring)\n",
    "        self.training_score = self.model.score(self.X_train_new, self.y_train_new)\n",
    "\n",
    "        # predict and get classification report & confusion matrix info\n",
    "        self.predictions = self.model.predict(self.X_test)\n",
    "        self.class_report = classification_report(self.y_test, self.predictions, output_dict= True)\n",
    "        self.tn, self.fp, self.fn, self.tp = confusion_matrix(self.y_test, self.predictions).ravel()\n",
    "        \n",
    "        self.model_tracker()\n",
    "        \n",
    "    def model_tracker(self):\n",
    "        self.results = pd.DataFrame({'Model': self.name, 'Method': self.method, 'cv_score': self.scores.mean(), \n",
    "                                'training_score': self.training_score,'accuracy': self.class_report[\"accuracy\"],\n",
    "                                'precision': self.class_report[\"1.0\"][\"precision\"], 'recall': self.class_report[\"1.0\"][\"recall\"],\n",
    "                                'f1-score': self.class_report[\"1.0\"][\"f1-score\"], 'TP': self.tp, 'FP': self.fp,\n",
    "                                'TN': self.tn, 'FN': self.fn}, index=[len(model_tracker.index)])\n",
    "        self.tracker =  pd.concat([self.tracker, self.results])\n",
    "    \n",
    "    def run_all(self):\n",
    "        self.random_under()\n",
    "        \n",
    "        self.tomeklinks()\n",
    "        self.neighbourhoodclean()\n",
    "        self.nearmiss()\n",
    "        self.smote_upsample()\n",
    "        self.random_over()\n",
    "        self.adasyn()\n",
    "        return self.tracker\n",
    "    \n",
    "    def random_under(self):\n",
    "        \"\"\"Randomly undersamples the predictor class. Can lead to loss of information \n",
    "        unless the majority class is relatively uniform\"\"\"\n",
    "        \n",
    "        self.sampler = RandomUnderSampler(sampling_strategy= self.sampling_strategy, random_state= 13)\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train , self.y_test = train_test_split( \\\n",
    "            self.X, self.y, train_size=self.train_size, stratify=self.stratify, random_state=13)\n",
    "        self.X_train_new, self.y_train_new = self.sampler.fit_resample(self.X_train, self.y_train)\n",
    "        self.method = \"Random Under\"\n",
    "        self.model_runner() \n",
    "        print(\"Random undersample run complete\")\n",
    "        return self.tracker\n",
    "    \n",
    "    def clustercentroids(self):\n",
    "        \"\"\"This method undersamples the majority class by replacing a cluster of majority samples. \n",
    "        Clusters of majority class found with K-mean algorithms. Then it keeps the cluster centroids of the \n",
    "        N clusters as the new majority samples\"\"\"\n",
    "        self.sampler = ClusterCentroids(sampling_strategy= self.sampling_strategy, random_state= 13)\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train , self.y_test = train_test_split(self.X, self.y, train_size=self.train_size, stratify=self.stratify, random_state=13)\n",
    "        self.X_train_new, self.y_train_new = self.sampler.fit_resample(self.X_train, self.y_train)\n",
    "        self.method = \"Cluster Centroids\"\n",
    "        self.model_runner()\n",
    "        \n",
    "        print(\"Cluster centroids run complete\")\n",
    "        return self.tracker\n",
    "    \n",
    "    def tomeklinks(self):\n",
    "        \"\"\"Finds samples near the borderline of the two classess. Given two instances, a & b seperated by distance\n",
    "        d(a,b) the pair is called a Tomek link if there is no instance c such that d(a,c) < d(a,b) or d(b,c) < d(a,b)\n",
    "        Instances within Tomek links are considered noise or borderline and are thus removed\"\"\"\n",
    "        \n",
    "        self.sampler = TomekLinks()\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train , self.y_test = train_test_split(self.X, self.y, train_size=self.train_size, stratify=self.stratify, random_state=13)\n",
    "        self.X_train_new, self.y_train_new = self.sampler.fit_resample(self.X_train, self.y_train)\n",
    "        self.method = \"Tomek Links\"\n",
    "        self.model_runner()\n",
    "        \n",
    "        print(\"TomekLinks run complete\")\n",
    "        return self.tracker\n",
    "    \n",
    "    def neighbourhoodclean(self):\n",
    "        \"\"\" Edited Nearest Neighbor Rule (ENN) to remove any instance whose class label is different from the class\n",
    "        of at least two of its three nearest neighbors. Neighbourhood cleaning rule uses ENN to remove majority samples\n",
    "        Finds three nearest neighbors for each training set instance, if majority class and opposite to its neighbours \n",
    "        it is removed. If belongs to the target class than the neighbours are removed\"\"\"\n",
    "        \n",
    "        self.sampler = NeighbourhoodCleaningRule()\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train , self.y_test = train_test_split(self.X, self.y, train_size=self.train_size, stratify=self.stratify, random_state=13)\n",
    "        self.X_train_new, self.y_train_new = self.sampler.fit_resample(self.X_train, self.y_train)\n",
    "        self.method = \"Neighbourhood Clean\"\n",
    "        self.model_runner()\n",
    "        \n",
    "        print(\"Neighbourhood Cleaning run complete\")\n",
    "        return self.tracker\n",
    "    \n",
    "    def nearmiss(self):\n",
    "        \"\"\"Calculates distances between all instance of majority and minority classes. K instances of the majority\n",
    "        class with smallest distances to minority are selected and removed\"\"\"\n",
    "        \n",
    "        self.sampler = NearMiss(sampling_strategy= self.sampling_strategy)\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train , self.y_test = train_test_split(self.X, self.y, train_size=self.train_size, stratify=self.stratify, random_state=13)\n",
    "        self.X_train_new, self.y_train_new = self.sampler.fit_resample(self.X_train, self.y_train)\n",
    "        self.method = \"Near Miss\"\n",
    "        self.model_runner()\n",
    "       \n",
    "        print(\"Near Miss run complete\")\n",
    "        return self.tracker\n",
    "        \n",
    "    \n",
    "    def smote_upsample(self, k_neighbors = 5):\n",
    "        self.k_neighbors = k_neighbors\n",
    "        self.sampler = SMOTE(sampling_strategy= self.sampling_strategy, random_state= 13, k_neighbors= self.k_neighbors)\n",
    "        self.X_train, self.X_test, self.y_train , self.y_test = train_test_split(self.X, self.y, train_size=self.train_size, stratify=self.stratify, random_state=13)\n",
    "\n",
    "        cv = KFold(n_splits=5, shuffle = True)\n",
    "        self.scores = np.array([])\n",
    "        for train_fold_index, val_fold_index in cv.split(self.X_train, self.y_train):\n",
    "         \n",
    "             # Get the training data\n",
    "            X_train_fold, y_train_fold = self.X_train[train_fold_index], self.y_train[train_fold_index]\n",
    "        # Get the validation data\n",
    "            X_val_fold, y_val_fold = self.X_train[val_fold_index], self.y_train[val_fold_index]\n",
    "\n",
    "        # Upsample only the data in the training section\n",
    "            X_train_fold_upsample, y_train_fold_upsample = self.sampler.fit_resample(X_train_fold,\n",
    "                                                                           y_train_fold)\n",
    "            model_obj = self.model.fit(X_train_fold_upsample, y_train_fold_upsample)\n",
    "            score = recall_score(y_val_fold, model_obj.predict(X_val_fold))\n",
    "            self.scores = np.append(self.scores, score)  \n",
    "        \n",
    "        self.X_train_new, self.y_train_new = self.sampler.fit_resample(self.X_train, self.y_train)\n",
    "        self.model.fit(self.X_train_new, self.y_train_new)\n",
    "        self.training_score = self.model.score(self.X_train_new, self.y_train_new)\n",
    "\n",
    "        # predict and get classification report & confusion matrix info\n",
    "        self.predictions = self.model.predict(self.X_test)\n",
    "        self.class_report = classification_report(self.y_test, self.predictions, output_dict= True)\n",
    "        self.tn, self.fp, self.fn, self.tp = confusion_matrix(self.y_test, self.predictions).ravel()\n",
    "        self.method = \"SMOTE\"\n",
    "        self.model_tracker()\n",
    "        \n",
    "        print(\"SMOTE run complete\")\n",
    "        return self.tracker\n",
    "    \n",
    "    def random_over(self):\n",
    "        self.sampler = RandomOverSampler(sampling_strategy= self.sampling_strategy, random_state= 13)\n",
    "        self.X_train, self.X_test, self.y_train , self.y_test = train_test_split(self.X, self.y, train_size=self.train_size, stratify=self.stratify, random_state=13)\n",
    "\n",
    "        cv = KFold(n_splits=5, shuffle = True)\n",
    "        self.scores = np.array([])\n",
    "        for train_fold_index, val_fold_index in cv.split(self.X_train, self.y_train):\n",
    "         \n",
    "             # Get the training data\n",
    "            X_train_fold, y_train_fold = self.X_train[train_fold_index], self.y_train[train_fold_index]\n",
    "        # Get the validation data\n",
    "            X_val_fold, y_val_fold = self.X_train[val_fold_index], self.y_train[val_fold_index]\n",
    "\n",
    "        # Upsample only the data in the training section\n",
    "            X_train_fold_upsample, y_train_fold_upsample = self.sampler.fit_resample(X_train_fold,\n",
    "                                                                           y_train_fold)\n",
    "            model_obj = self.model.fit(X_train_fold_upsample, y_train_fold_upsample)\n",
    "            score = recall_score(y_val_fold, model_obj.predict(X_val_fold))\n",
    "            self.scores = np.append(self.scores, score)  \n",
    "        \n",
    "        self.X_train_new, self.y_train_new = self.sampler.fit_resample(self.X_train, self.y_train)\n",
    "        self.model.fit(self.X_train_new, self.y_train_new)\n",
    "        self.training_score = self.model.score(self.X_train_new, self.y_train_new)\n",
    "      \n",
    "\n",
    "        # predict and get classification report & confusion matrix info\n",
    "        self.predictions = self.model.predict(self.X_test)\n",
    "        self.class_report = classification_report(self.y_test, self.predictions, output_dict= True)\n",
    "        self.tn, self.fp, self.fn, self.tp = confusion_matrix(self.y_test, self.predictions).ravel()\n",
    "        self.method = \"Random oversample\"\n",
    "        self.model_tracker()\n",
    "        \n",
    "        print(\"Random oversample run complete\")\n",
    "        return self.tracker\n",
    "        \n",
    "\n",
    "        \n",
    "    def adasyn(self, n_neighbors = 5):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.sampler = ADASYN(sampling_strategy= self.sampling_strategy, random_state= 13, n_neighbors= self.n_neighbors)\n",
    "        self.X_train, self.X_test, self.y_train , self.y_test = train_test_split(self.X, self.y, train_size=self.train_size, stratify=self.stratify, random_state=13)\n",
    "\n",
    "        cv = KFold(n_splits=5, shuffle = True)\n",
    "        self.scores = np.array([])\n",
    "        for train_fold_index, val_fold_index in cv.split(self.X_train, self.y_train):\n",
    "         \n",
    "             # Get the training data\n",
    "            X_train_fold, y_train_fold = self.X_train[train_fold_index], self.y_train[train_fold_index]\n",
    "        # Get the validation data\n",
    "            X_val_fold, y_val_fold = self.X_train[val_fold_index], self.y_train[val_fold_index]\n",
    "\n",
    "        # Upsample only the data in the training section\n",
    "            X_train_fold_upsample, y_train_fold_upsample = self.sampler.fit_resample(X_train_fold,\n",
    "                                                                           y_train_fold)\n",
    "            model_obj = self.model.fit(X_train_fold_upsample, y_train_fold_upsample)\n",
    "            score = recall_score(y_val_fold, model_obj.predict(X_val_fold))\n",
    "            self.scores = np.append(self.scores, score)  \n",
    "        \n",
    "        self.X_train_new, self.y_train_new = self.sampler.fit_resample(self.X_train, self.y_train)\n",
    "        self.model.fit(self.X_train_new, self.y_train_new)\n",
    "        self.training_score = self.model.score(self.X_train_new, self.y_train_new)\n",
    "\n",
    "        # predict and get classification report & confusion matrix info\n",
    "        self.predictions = self.model.predict(self.X_test)\n",
    "        self.class_report = classification_report(self.y_test, self.predictions, output_dict= True)\n",
    "        self.tn, self.fp, self.fn, self.tp = confusion_matrix(self.y_test, self.predictions).ravel()\n",
    "        self.method = \"ADASYN\"\n",
    "        self.model_tracker()\n",
    "        \n",
    "        print(\"ADASYN run complete\")\n",
    "        return self.tracker\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prioritising for Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.806777Z",
     "start_time": "2020-06-03T16:11:06.582Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dataframe to track params & scoring\n",
    "cols = [\"Model\", \"Method\", \"cv_score\", \"training_score\", \"accuracy\", \"precision\", \"recall\", \"f1-score\", \"TP\", \"FP\", \"TN\", \"FN\"]\n",
    "model_tracker = pd.DataFrame(columns= cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.807965Z",
     "start_time": "2020-06-03T16:11:06.587Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit log reg instance of tiny target\n",
    "tt = tiny_target(X,y, model= LogisticRegression(solver = \"sag\", max_iter = 10000), tracker = model_tracker, name = \"log_reg\")\n",
    "model_tracker = model_tracker.append(tt.run_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.809969Z",
     "start_time": "2020-06-03T16:11:06.588Z"
    }
   },
   "outputs": [],
   "source": [
    "# fit KNN instance of tiny target\n",
    "tt = tiny_target(X,y, model= KNeighborsClassifier(), tracker = model_tracker, name = \"KNN\")\n",
    "model_tracker = model_tracker.append(tt.run_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.811433Z",
     "start_time": "2020-06-03T16:11:06.590Z"
    }
   },
   "outputs": [],
   "source": [
    "# fit decision tree instance of tiny target\n",
    "tt = tiny_target(X,y, model= DecisionTreeClassifier(min_samples_split= 50), tracker = model_tracker, name = \"Dec_Tree\")\n",
    "model_tracker = model_tracker.append(tt.run_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.813036Z",
     "start_time": "2020-06-03T16:11:06.592Z"
    }
   },
   "outputs": [],
   "source": [
    "# fit random forest instance of tiny target\n",
    "tt = tiny_target(X,y, model= RandomForestClassifier(n_estimators= 100, n_jobs= 2), tracker = model_tracker, name = \"Ran_FR\")\n",
    "model_tracker = model_tracker.append(tt.run_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.814018Z",
     "start_time": "2020-06-03T16:11:06.594Z"
    }
   },
   "outputs": [],
   "source": [
    "# fit grad boost instance of tiny target\n",
    "tt = tiny_target(X,y, model= GradientBoostingClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=1), tracker = model_tracker, name = \"GB\")\n",
    "model_tracker = model_tracker.append(tt.run_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.814913Z",
     "start_time": "2020-06-03T16:11:06.596Z"
    }
   },
   "outputs": [],
   "source": [
    "# fit multinomial bayes instance of tiny target\n",
    "tt = tiny_target(X,y, model= MultinomialNB(), tracker = model_tracker, name = \"M_NB\")\n",
    "model_tracker = model_tracker.append(tt.run_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.816130Z",
     "start_time": "2020-06-03T16:11:06.599Z"
    }
   },
   "outputs": [],
   "source": [
    "model_tracker.to_csv(\"/Users/samholt/GA/DSI12-lessons/projects/Capstone_Project/Capstone_MKII_Model_tracker.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.817315Z",
     "start_time": "2020-06-03T16:11:06.601Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_tracker.sort_values(by = \"recall\", ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.818658Z",
     "start_time": "2020-06-03T16:11:06.603Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualisation of accuracy vs. recall for these models show a distinct trade-off, the model(s) to\n",
    "# take forward are the ones which minimize the accuracy recall trade-off\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "sns.scatterplot(model_tracker[\"recall\"], model_tracker[\"accuracy\"], hue= model_tracker[\"Model\"])\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.819940Z",
     "start_time": "2020-06-03T16:11:06.604Z"
    }
   },
   "outputs": [],
   "source": [
    "#select models which maximize both accuracy & recall, defined as 80% accuracy and 50% recall\n",
    "model_tracker[(model_tracker[\"recall\"] > 0.49) & (model_tracker[\"accuracy\"] > 0.79)][[\"Model\", \"Method\", \"accuracy\", \"recall\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gridsearch & Optimize Best Model\n",
    "* Due to time constraints one logistic regression model was taken forward for analysis & gridsearch on the full dataset\n",
    "* Gridsearching based on our best model, logistic regression combined with random undersampling\n",
    "* Random undersampling loses a lot of data so we are using the model on the full sparse DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.821752Z",
     "start_time": "2020-06-03T16:11:06.606Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split sample into target and predictor variables\n",
    "X = sparse_df[:,:-1]\n",
    "y = sparse_df[:, -1].toarray().ravel()\n",
    "\n",
    "X\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.823030Z",
     "start_time": "2020-06-03T16:11:06.610Z"
    }
   },
   "outputs": [],
   "source": [
    "# fit sampler\n",
    "\n",
    "sampler = RandomUnderSampler(sampling_strategy= 0.5, random_state= 13)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.3, stratify = y, random_state = 13)\n",
    "\n",
    "X_train_new, y_train_new = sampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.824333Z",
     "start_time": "2020-06-03T16:11:06.612Z"
    }
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver = \"sag\", max_iter = 10000)\n",
    "params = {'C':[0.001, 0.01, 1, 10,25],\n",
    "          'fit_intercept': [True, False]}\n",
    "\n",
    "model_GS = GridSearchCV(estimator=model,\n",
    "                param_grid=params,\n",
    "                cv=5,\n",
    "                scoring='recall',\n",
    "                return_train_score=True, verbose= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.825823Z",
     "start_time": "2020-06-03T16:11:06.614Z"
    }
   },
   "outputs": [],
   "source": [
    "# fit model and assess classification report\n",
    "model_GS.fit(X_train_new,y_train_new)\n",
    "print(\"Baseline testing score was: {}\".format(str(baseline)))\n",
    "print(\"Logistic regression best mean cross val score was: {}\".format(str(cross_val_score(model_GS, X_train_new, y_train_new, cv=5).mean())))\n",
    "print(\"Logistic regression best training score was : {}\".format(str(model_GS.score(X_train_new, y_train_new))))\n",
    "predictions = model_GS.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.826827Z",
     "start_time": "2020-06-03T16:11:06.616Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.830417Z",
     "start_time": "2020-06-03T16:11:06.618Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "plot_confusion_matrix(model_GS,X_test, y_test, ax = ax, values_format=\"\")\n",
    "plt.ylim()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.831723Z",
     "start_time": "2020-06-03T16:11:06.619Z"
    }
   },
   "outputs": [],
   "source": [
    "# collect the model coefficients in a dataframe\n",
    "df_coef = pd.DataFrame(model_GS.best_estimator_.coef_, columns= unique_subs[:-1],\n",
    "                       index=['coefficients'])\n",
    "# calculate the absolute values of the coefficients\n",
    "df_coef = df_coef.T\n",
    "df_coef['coef_abs'] = df_coef.coefficients.abs()\n",
    "df_coef.sort_values(by = \"coef_abs\", ascending = False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:11:37.833007Z",
     "start_time": "2020-06-03T16:11:06.621Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize = (8, 13))\n",
    "df_coef['coef_abs'].sort_values(ascending = False).head(15).plot(kind='barh');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "* Overall the evaluated models offer potential for identifying mental health risk in internet forum users\n",
    "* In particular, the greatest potential use case is likely to be identifying users who may not currently be aware of, and post to, support forums but who could benefit from their supports & thus promotion on over popular subreddits\n",
    "* The high recall at the expense at accuracy is too much in some cases, though a high number of false positives is not as much of an issue for a risk analyses, and indeed may reflect current sufferers of mental health issues who simply do not post to reddit about their issues\n",
    "* Their are some potential risks & limitations for this research:\n",
    "    * The demographic used in the sample is likely to be younger & focused on North America & Europe \n",
    "    * The majority of users on any website are â€˜lurkersâ€™ and do not post, this research assumes that they are represented & do not differ significantly in personality etc. than those who do post\n",
    "    * Key assumption is that posting on mental health forums is indicative of mental health risk \n",
    "* Next steps would include:\n",
    "    * Grid Searching & fine tuning remaining models\n",
    "    * Expanding data set to incorporate additional years for greater breadth of sample\n",
    "    * Network analysis to flesh out those subreddits most closely related to mental health \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "378.212px",
    "left": "1755.56px",
    "right": "20px",
    "top": "120px",
    "width": "357.778px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
