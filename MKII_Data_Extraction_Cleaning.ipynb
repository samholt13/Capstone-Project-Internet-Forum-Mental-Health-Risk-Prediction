{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.189Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import & Initial Munging\n",
    "* Data was pulled from the bigquery pushshift.io dataset via SQL, with author, subreddit and count of number of posts from 2018\n",
    "* Full SQL query:\n",
    "    * SELECT author, subreddit, count(subreddit) FROM [fh-bigquery.reddit_posts.2018_12] WHERE author != \"[deleted]\" AND subreddit IS NOT NULL GROUP BY author, subreddit\n",
    "* Included all months for 2018 within SQL query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.196Z"
    }
   },
   "outputs": [],
   "source": [
    "#import data, 3 files with all 2018 posts split into between\n",
    "raw_data_1 = pd.read_csv('/Users/samholt/GA/Capstone_Data_MKII/Capstone_MKII_Data_1.csv', sep = \",\")\n",
    "raw_data_2 = pd.read_csv('/Users/samholt/GA/Capstone_Data_MKII/Capstone_MKII_Data_2.csv', sep = \",\")\n",
    "raw_data_3 = pd.read_csv('/Users/samholt/GA/Capstone_Data_MKII/Capstone_MKII_Data_3.csv', sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.200Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge dataframe\n",
    "df_first = pd.merge(raw_data_1, raw_data_2, on=[\"author\", \"subreddit\"], how= \"outer\" )\n",
    "df= pd.merge(raw_data_3, df_first, on=[\"author\", \"subreddit\"], how= \"outer\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.202Z"
    }
   },
   "outputs": [],
   "source": [
    "#  checking to see how the merge has gone, need to deal with null values\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.204Z"
    }
   },
   "outputs": [],
   "source": [
    "#dealing with null values with 0 as they mean a user hasn't posted to that subreddit in one of the three periods\n",
    "df.fillna(0, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.207Z"
    }
   },
   "outputs": [],
   "source": [
    "# create one variable with overall count of posts per subreddit per user\n",
    "df[\"posts\"] = df[\"f0_\"] + df[\"f0__x\"] + df[\"f0__y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.209Z"
    }
   },
   "outputs": [],
   "source": [
    "#drop the remaining variables\n",
    "df.drop(labels= ['f0_', 'f0__x', 'f0__y'], inplace= True, axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.211Z"
    }
   },
   "outputs": [],
   "source": [
    "#check for null values, \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.213Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.214Z"
    }
   },
   "outputs": [],
   "source": [
    "num_authors = len(df[\"author\"].unique())\n",
    "num_subreddits = len(df[\"subreddit\"].unique())\n",
    "num_posts = df[\"posts\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.216Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = [num_authors, num_subreddits, num_posts]\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "plt.bar(x = [\"Users\", \"Subreddits\", \"Posts\"], height= labels, log = True)\n",
    "plt.title(\"Unique Counts within Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Steps\n",
    "* Converting authors to numerical key, saves memory & not interested in specific users\n",
    "* Dealing with outliers in number of posts per user\n",
    "* Understanding subreddit usage per user & removing those with limited information (number of subreddits posted to)\n",
    "* Creating target variable\n",
    "    * Boolean for whether a user posted to list of mental health related subreddits\n",
    "* Merging single user subreddits and those with a small number of users in to one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Authors to Numerical Key\n",
    "* As we don't want to delve into individual tastes, and to save on any issues with memory converting authors to a numerical key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.218Z"
    }
   },
   "outputs": [],
   "source": [
    "# get list of unique authors\n",
    "authors = df[\"author\"].unique()\n",
    "len(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.220Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating dictionary in order to assign numerical value per author\n",
    "authors_dict = {}\n",
    "counter = 0\n",
    "for i in authors:\n",
    "    authors_dict[i] = counter\n",
    "    counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.221Z"
    }
   },
   "outputs": [],
   "source": [
    "# replace authors with created numerical key\n",
    "df[\"author\"] = [authors_dict[i] for i in df[\"author\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for outliers\n",
    "* Visualisation of the data shows a number of outliers in terms of number of posts per user; high posters were often bots or spam and were removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.223Z"
    }
   },
   "outputs": [],
   "source": [
    "# every user has posted at least once to a specific subreddit, though seem to be a curiously high number of posts in some instances\n",
    "pd.set_option('float_format', '{:f}'.format)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.224Z"
    }
   },
   "outputs": [],
   "source": [
    "# check distribution of data, we see a high number of outliers which indicate strange behaviour\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "sns.boxplot(data=df[\"posts\"], orient='h', fliersize=5, linewidth=3,\n",
    "            saturation=0.5, ax=ax)\n",
    "\n",
    "ax.set_title('Number of Posts per User, Pre-Outlier Removal')\n",
    "ax.set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.226Z"
    }
   },
   "outputs": [],
   "source": [
    "# we see there are a lot of bots included with these outliers which we will need to remove\n",
    "df.sort_values(by = \"posts\", axis= 0, ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.230Z"
    }
   },
   "outputs": [],
   "source": [
    "df_mean = df[\"posts\"].mean()\n",
    "df_std = df[\"posts\"].std()\n",
    "df_outliers = df[df[\"posts\"] > df_mean + (df_std * 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.231Z"
    }
   },
   "outputs": [],
   "source": [
    "df_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.235Z"
    }
   },
   "outputs": [],
   "source": [
    "# as a large number of posters significantly higher than the mean appear to be bots or marketers, dropping them from the dataset\n",
    "df = df[df[\"posts\"] < df_mean + (df_std * 4)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.237Z"
    }
   },
   "outputs": [],
   "source": [
    "# after dropping outliers we see a more even distribution of data, though still a large number of outliers\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "sns.boxplot(data=df[\"posts\"], orient='h', fliersize=5, linewidth=3,\n",
    "            saturation=0.5, ax=ax)\n",
    "\n",
    "ax.set_title('Number of Posts per User, Post-Outlier Removal')\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understand Subreddit Usage per User\n",
    "* Additionally, the majority of users only post to a small number of subreddits, any user who posts to less than 10 subreddits was dropped from the analyses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.239Z"
    }
   },
   "outputs": [],
   "source": [
    "# log scale\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "df[\"author\"].value_counts().plot(kind='hist', bins=4000, title='Number of Subreddits per User')\n",
    "plt.xlabel('Author')\n",
    "plt.ylabel('Frequency')\n",
    "ax.set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.241Z"
    }
   },
   "outputs": [],
   "source": [
    "#get dict of unique counts \n",
    "author_count = df[\"author\"].value_counts().to_dict()\n",
    "\n",
    "# add column for frequency of users\n",
    "df[\"freq\"] = [author_count[i] for i in df[\"author\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.243Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop users who have posted to less than 10 subreddits, will not be useful for analysis\n",
    "df = df[df[\"freq\"] > 9].copy()\n",
    "\n",
    "# drop frequncy column\n",
    "df.drop(labels = \"freq\", axis= 1, inplace= True)\n",
    "\n",
    "# del author_count to save memory\n",
    "del author_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.249Z"
    }
   },
   "outputs": [],
   "source": [
    "# log scale\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "df[\"author\"].value_counts().plot(kind='hist', bins=4000, title='Number of Subreddits per User')\n",
    "plt.xlabel('Author')\n",
    "plt.ylabel('Frequency')\n",
    "ax.set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine mental health related subreddits for target variable\n",
    "* Create target variable\n",
    "* Turn into boolean, 1 for posting to mental health subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.251Z"
    }
   },
   "outputs": [],
   "source": [
    "#target reddits identified from r/anxiety related subreddits\n",
    "string= \"\"\"/r/mentalhealth/r/Anxiety/r/SuicideWatch/r/bipolar/r/bipolarreddit/r/depression/r/detachmentdisorder/r/dpdr/r/GFD/r/MentalHealth/r/mentalillness/r/stopselfharm/r/Agoraphobia/r/anxietydepression/r/HealthAnxiety/r/socialanxiety/r/Anxietyhelp/r/BPD\"\"\"\n",
    "target_reddits = string.split(\"/r/\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.252Z"
    }
   },
   "outputs": [],
   "source": [
    "#get list of unique subreddits\n",
    "subreddits = df[\"subreddit\"].unique()\n",
    "len(subreddits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.255Z"
    }
   },
   "outputs": [],
   "source": [
    "# group together target variable\n",
    "df[\"subreddit\"] = [\"df_target\" if i in target_reddits else i for i in df[\"subreddit\"]]\n",
    "\n",
    "df[df[\"subreddit\"] == \"df_target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.257Z"
    }
   },
   "outputs": [],
   "source": [
    "#turn to boolean for prediction\n",
    "df.loc[df[\"subreddit\"] == \"df_target\", [\"posts\"]] = 1 \n",
    "df[df[\"subreddit\"] == \"df_target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.258Z"
    }
   },
   "outputs": [],
   "source": [
    "#check for duplicates, boolean now so can remove\n",
    "df[df[\"subreddit\"] == \"df_target\"].duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.264Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.267Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df[\"subreddit\"] == \"df_target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Tiny Subreddits\n",
    "* High number of subreddits have less than 10 posters, often related to personal usernames, merging these into one feature \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.269Z"
    }
   },
   "outputs": [],
   "source": [
    "# we see over 1 million subreddits which only have one user, merging these into \"single_user_reddits\"\n",
    "single_df = df.groupby(\"subreddit\").count()[df.groupby(\"subreddit\").count()[\"author\"] < 11]\n",
    "single_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.270Z"
    }
   },
   "outputs": [],
   "source": [
    "#create list of subreddits to merge\n",
    "single_user_subreddits = single_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-02T18:48:25.272Z"
    }
   },
   "outputs": [],
   "source": [
    "# renaming single user subreddits to one variable\n",
    "df[\"subreddit\"] = [\"single_user_subreddits\" if i in single_user_subreddits else i for i in df[\"subreddit\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T11:10:51.268362Z",
     "start_time": "2020-06-02T11:10:08.751933Z"
    }
   },
   "outputs": [],
   "source": [
    "#final file to take through for EDA\n",
    "df.to_csv(\"/Users/samholt/GA/DSI12-lessons/projects/Capstone_Project/Capstone_MKII_DataFinal.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
